--------------------------------------------------------------------------
WARNING: There is at least one OpenFabrics device found but there are
no active ports detected (or Open MPI was unable to use them).  This
is most certainly not what you wanted.  Check your cables, subnet
manager configuration, etc.  The openib BTL will be ignored for this
job.

  Local host: n5
--------------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/jason/pypar/parabola/test.py", line 8, in <module>
    parallel_solve_problem(problem.initialize, problem.func, problem.finalize, my_rank, num_procs, pypar.send, pypar.receive)
  File "/home/jason/pypar/parabola/Parabola.py", line 29, in parallel_solve_problem
    my_input_args = get_subproblem_input_args(input_args, my_rank, num_procs)
  File "/home/jason/pypar/parabola/Parabola.py", line 10, in get_subproblem_input_args
    sub_ns = simple_partitioning(len(input_args), num_procs)
  File "/home/jason/pypar/parabola/Parabola.py", line 4, in simple_partitioning
    sublengths = [lenth/num_procs]*num_procs
NameError: global name 'lenth' is not defined
--------------------------------------------------------------------------
mpirun has exited due to process rank 0 with PID 38699 on
node n5 exiting without calling "finalize". This may
have caused other processes in the application to be
terminated by signals sent by mpirun (as reported here).
--------------------------------------------------------------------------
